{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "# CIFAR-10 Diffusion - RGB Images Here We Come! ðŸš€\n\nTime to level up from grayscale MNIST to full-color CIFAR-10! This notebook demonstrates our generic diffusion pipeline working on **32Ã—32 RGB images** with minimal changes to the architecture."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load the CIFAR-10 Dataset\n",
    "\n",
    "CIFAR-10 contains 50,000 training images across 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. Each image is **32Ã—32 pixels with 3 color channels (RGB)**."
   ],
   "id": "279b7e7ba0cae3e"
  },
  {
   "cell_type": "code",
   "id": "7qg4dolm0f",
   "source": "import torch\nimport matplotlib.pyplot as plt\nfrom torchvision import datasets, transforms\n\nfrom src.diffusion_playground.diffusion.noise_schedule import LinearNoiseSchedule\nfrom src.diffusion_playground.diffusion.training_utils import sample_xt\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a1pqxwljtjt",
   "source": "## Explore the Dataset",
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load CIFAR-10 dataset directly as a tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize RGB channels to [-1, 1]\n",
    "])\n",
    "\n",
    "cifar_dataset = datasets.CIFAR10(root=\"data\", train=True, transform=transform, download=True)\n",
    "\n",
    "# Extract all images into a single tensor\n",
    "cifar_data = torch.stack([cifar_dataset[i][0] for i in range(len(cifar_dataset))])\n",
    "cifar_labels = torch.tensor([cifar_dataset[i][1] for i in range(len(cifar_dataset))])\n",
    "\n",
    "print(f\"Dataset shape: {cifar_data.shape}\")\n",
    "print(f\"Labels shape: {cifar_labels.shape}\")\n",
    "print(f\"Input shape per image: (3, 32, 32)\")\n",
    "print(f\"\\nClasses: {cifar_dataset.classes}\")"
   ],
   "id": "6a841a1143df08cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8z8g617brnh",
   "source": [
    "# Visualize a few random samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    sample_idx = torch.randint(0, len(cifar_data), (1,)).item()\n",
    "\n",
    "    # Convert from [-1, 1] to [0, 1] for visualization\n",
    "    img = (cifar_data[sample_idx].permute(1, 2, 0) + 1) / 2\n",
    "    img = torch.clamp(img, 0, 1)  # Ensure values are in valid range\n",
    "\n",
    "    ax.imshow(img.cpu())\n",
    "    ax.set_title(f\"{cifar_dataset.classes[cifar_labels[sample_idx]]}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Sample CIFAR-10 Images\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "y2iwdptke9l",
   "source": "## Forward Diffusion on CIFAR-10\n\nLet's visualize how images progressively become noise during the forward diffusion process. We'll see the same image at different time steps from clean (t=0) to pure noise (t=999).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "8htsy8eaocn",
   "source": [
    "# Create NoiseSchedule\n",
    "schedule = LinearNoiseSchedule(time_steps=1_000)\n",
    "\n",
    "# Use a batch from our data for visualization\n",
    "x0_batch = cifar_data[:128]  # Take first 128 samples\n",
    "\n",
    "time_steps = [0, 50, 200, 500, 999]\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i, t in enumerate(time_steps):\n",
    "    # Create a batch of time steps (one per sample in the batch)\n",
    "    t_tensor = torch.full((x0_batch.shape[0],), t)\n",
    "    xt, _, _ = sample_xt(x0_batch, schedule, t=t_tensor)\n",
    "\n",
    "    # Convert from [-1, 1] to [0, 1] for visualization\n",
    "    img = (xt[0].permute(1, 2, 0) + 1) / 2\n",
    "    img = torch.clamp(img, 0, 1)\n",
    "\n",
    "    plt.subplot(1, len(time_steps), i + 1)\n",
    "    plt.imshow(img.cpu())\n",
    "    plt.title(f\"t={t}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Forward Diffusion Process (Clean â†’ Noise)\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "tayy0m3hy69",
   "source": "## Train the CNN Denoiser\n\nNow let's train our UNet-style CNN to learn the reverse diffusion process on CIFAR-10! \n\n**Key differences from MNIST:**\n- **Input channels**: 3 (RGB) instead of 1 (grayscale)\n- **Base channels**: 64 instead of 32 (more capacity for complex color images)\n- **Image size**: 32Ã—32 instead of 28Ã—28\n\nEverything else stays the same - same training pipeline, same loss function, same noise schedule! ðŸŽ¯",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "hj3gm3gtqh",
   "source": "from src.diffusion_playground.models import CNNDenoiser\nfrom src.diffusion_playground.training.denoiser_trainer import train_denoiser\n\n# Data is already loaded as cifar_data tensor!\nprint(f\"Training data shape: {cifar_data.shape}\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "s1s0cp6m2e",
   "source": [
    "# Create the CNN denoiser model for RGB images\n",
    "model = CNNDenoiser(\n",
    "    in_channels=3,  # RGB (3 channels) instead of grayscale (1 channel)\n",
    "    base_channels=64,  # More capacity than MNIST (was 32)\n",
    "    time_emb_dim=128  # Same time embedding dimension\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dbrlzg5vy8a",
   "source": [
    "# Train the model (using the same generic training function!)\n",
    "# Note: CIFAR-10 is more complex than MNIST, may need longer training\n",
    "# The training will automatically RESUME from the latest checkpoint if interrupted!\n",
    "train_denoiser(\n",
    "    model=model,\n",
    "    data=cifar_data,\n",
    "    noise_schedule=schedule,\n",
    "    epochs=100,\n",
    "    lr=1e-3,\n",
    "    batch_size=128,\n",
    "    checkpoint_dir=\"checkpoints/cifar10_cnn\",\n",
    "    save_every=5,\n",
    "    resume=True\n",
    ")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "v2li4bse91",
   "source": "### ðŸ”„ Auto-Resume Feature\n\nThe training will **automatically resume** from the latest checkpoint if interrupted! This is perfect for:\n- â° Google Colab sessions that time out\n- ðŸ”Œ Unexpected disconnections\n- ðŸ›‘ Manual interruptions\n\nJust re-run the training cell and it will pick up where it left off. No manual checkpoint management needed!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "rzhqpscgrv",
   "source": "## Test the Reverse Diffusion Process\n\nLet's test our trained model by generating RGB images from pure noise! This will verify that all interfaces work correctly for CIFAR-10 before we move to long training on Google Colab.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "n35prg5ucj",
   "source": [
    "from src.diffusion_playground.training.denoiser_trainer import load_checkpoint\n",
    "\n",
    "# Load checkpoint for testing\n",
    "cp_name = \"best_model.pt\"\n",
    "checkpoint_path = f\"checkpoints/cifar10_cnn/{cp_name}\"\n",
    "\n",
    "# Create a fresh model instance\n",
    "model = CNNDenoiser(in_channels=3, base_channels=64, time_emb_dim=128)\n",
    "model.to(device)\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint_info = load_checkpoint(model, checkpoint_path, device=device)\n",
    "print(f\"Loaded model trained for {checkpoint_info['epoch']} epochs\")\n",
    "print(f\"Training loss: {checkpoint_info['loss']:.6f}\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "g24os5v85bm",
   "source": [
    "# Setup for generation\n",
    "model.eval()\n",
    "num_samples = 16\n",
    "time_steps = schedule.time_steps\n",
    "\n",
    "# Start from pure noise (RGB)\n",
    "xt = torch.randn(num_samples, 3, 32, 32).to(device)\n",
    "\n",
    "print(f\"Starting reverse diffusion from noise shape: {xt.shape}\")\n",
    "\n",
    "# Reverse diffusion loop with STOCHASTIC sampling (adds noise for diversity!)\n",
    "with torch.no_grad():\n",
    "    for t in reversed(range(1, time_steps + 1)):\n",
    "        # Create time tensor for all samples\n",
    "        t_tensor = torch.full((num_samples,), t, device=device, dtype=torch.long)\n",
    "\n",
    "        # Predict the noise\n",
    "        pred_noise = model(xt, t_tensor)\n",
    "\n",
    "        # Get schedule parameters\n",
    "        beta_t = schedule.betas[t - 1]\n",
    "        alpha_t = schedule.alphas[t - 1]\n",
    "        alpha_bar_t = schedule.alpha_bars[t - 1]\n",
    "\n",
    "        # Compute the mean of the reverse distribution\n",
    "        mean = (xt - beta_t / torch.sqrt(1 - alpha_bar_t) * pred_noise) / torch.sqrt(alpha_t)\n",
    "\n",
    "        # Add noise (STOCHASTIC sampling for diversity!)\n",
    "        if t > 1:\n",
    "            # Add Gaussian noise scaled by beta\n",
    "            noise = torch.randn_like(xt)\n",
    "            sigma_t = torch.sqrt(beta_t)\n",
    "            x_prev = mean + sigma_t * noise\n",
    "        else:\n",
    "            # No noise at the final step\n",
    "            x_prev = mean\n",
    "\n",
    "        # Update xt\n",
    "        xt = x_prev\n",
    "\n",
    "        # Print progress every 100 steps\n",
    "        if t % 100 == 0:\n",
    "            print(f\"  Step {time_steps - t + 1}/{time_steps} (t={t})\")\n",
    "\n",
    "print(\"Reverse diffusion complete!\")\n",
    "\n",
    "# Visualize generated RGB samples\n",
    "fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    # Convert from [-1, 1] to [0, 1] for visualization\n",
    "    img = (xt[idx].permute(1, 2, 0) + 1) / 2\n",
    "    img = torch.clamp(img, 0, 1)\n",
    "\n",
    "    ax.imshow(img.cpu())\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# Create informative title with checkpoint details\n",
    "title = f\"Generated CIFAR-10 Images - {cp_name}\\n\"\n",
    "title += f\"Epoch: {checkpoint_info['epoch']} | Loss: {checkpoint_info['loss']:.6f}\"\n",
    "plt.suptitle(title, fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c96f9e62bfdfffb4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
