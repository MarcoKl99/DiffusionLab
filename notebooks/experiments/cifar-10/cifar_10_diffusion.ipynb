{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "# CIFAR-10 Diffusion - RGB Images Here We Come! üöÄ\n\nTime to level up from grayscale MNIST to full-color CIFAR-10! This notebook demonstrates our generic diffusion pipeline working on **32√ó32 RGB images** with minimal changes to the architecture."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load the CIFAR-10 Dataset\n",
    "\n",
    "CIFAR-10 contains 50,000 training images across 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. Each image is **32√ó32 pixels with 3 color channels (RGB)**."
   ],
   "id": "279b7e7ba0cae3e"
  },
  {
   "cell_type": "code",
   "id": "7qg4dolm0f",
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from src.diffusion_playground.diffusion.backward import generate_samples\n",
    "from src.diffusion_playground.diffusion.noise_schedule import LinearNoiseSchedule\n",
    "from src.diffusion_playground.diffusion.training_utils import sample_xt\n",
    "from src.diffusion_playground.evaluation.image_generation_results import generate_samples_from_checkpoints\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a1pqxwljtjt",
   "source": "## Explore the Dataset",
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load CIFAR-10 dataset directly as a tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize RGB channels to [-1, 1]\n",
    "])\n",
    "\n",
    "cifar_dataset = datasets.CIFAR10(root=\"data\", train=True, transform=transform, download=False)\n",
    "\n",
    "# Extract all images into a single tensor\n",
    "cifar_data = torch.stack([cifar_dataset[i][0] for i in range(len(cifar_dataset))])\n",
    "cifar_labels = torch.tensor([cifar_dataset[i][1] for i in range(len(cifar_dataset))])\n",
    "\n",
    "print(f\"Dataset shape: {cifar_data.shape}\")\n",
    "print(f\"Labels shape: {cifar_labels.shape}\")\n",
    "print(f\"Input shape per image: (3, 32, 32)\")\n",
    "print(f\"\\nClasses: {cifar_dataset.classes}\")"
   ],
   "id": "6a841a1143df08cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8z8g617brnh",
   "source": [
    "# Visualize a few random samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    sample_idx = torch.randint(0, len(cifar_data), (1,)).item()\n",
    "\n",
    "    # Convert from [-1, 1] to [0, 1] for visualization\n",
    "    img = (cifar_data[sample_idx].permute(1, 2, 0) + 1) / 2\n",
    "    img = torch.clamp(img, 0, 1)  # Ensure values are in valid range\n",
    "\n",
    "    ax.imshow(img.cpu())\n",
    "    ax.set_title(f\"{cifar_dataset.classes[cifar_labels[sample_idx]]}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Sample CIFAR-10 Images\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "y2iwdptke9l",
   "source": "## Forward Diffusion on CIFAR-10\n\nLet's visualize how images progressively become noise during the forward diffusion process. We'll see the same image at different time steps from clean (t=0) to pure noise (t=999).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "8htsy8eaocn",
   "source": [
    "# Create NoiseSchedule\n",
    "schedule = LinearNoiseSchedule(time_steps=1_000)\n",
    "\n",
    "# Use a batch from our data for visualization\n",
    "x0_batch = cifar_data[:128]  # Take first 128 samples\n",
    "\n",
    "time_steps = [0, 50, 200, 500, 999]\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i, t in enumerate(time_steps):\n",
    "    # Create a batch of time steps (one per sample in the batch)\n",
    "    t_tensor = torch.full((x0_batch.shape[0],), t)\n",
    "    xt, _, _ = sample_xt(x0_batch, schedule, t=t_tensor)\n",
    "\n",
    "    # Convert from [-1, 1] to [0, 1] for visualization\n",
    "    img = (xt[0].permute(1, 2, 0) + 1) / 2\n",
    "    img = torch.clamp(img, 0, 1)\n",
    "\n",
    "    plt.subplot(1, len(time_steps), i + 1)\n",
    "    plt.imshow(img.cpu())\n",
    "    plt.title(f\"t={t}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Forward Diffusion Process (Clean ‚Üí Noise)\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "tayy0m3hy69",
   "source": "## Train the CNN Denoiser\n\nNow let's train our UNet-style CNN to learn the reverse diffusion process on CIFAR-10! \n\n**Key differences from MNIST:**\n- **Input channels**: 3 (RGB) instead of 1 (grayscale)\n- **Base channels**: 64 instead of 32 (more capacity for complex color images)\n- **Image size**: 32√ó32 instead of 28√ó28\n\nEverything else stays the same - same training pipeline, same loss function, same noise schedule! üéØ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "hj3gm3gtqh",
   "source": "from src.diffusion_playground.models import CNNDenoiser\nfrom src.diffusion_playground.training.denoiser_trainer import train_denoiser\n\n# Data is already loaded as cifar_data tensor!\nprint(f\"Training data shape: {cifar_data.shape}\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "s1s0cp6m2e",
   "source": [
    "# Create the CNN denoiser model for RGB images\n",
    "model = CNNDenoiser(\n",
    "    in_channels=3,  # RGB (3 channels) instead of grayscale (1 channel)\n",
    "    base_channels=64,  # More capacity than MNIST (was 32)\n",
    "    time_emb_dim=128  # Same time embedding dimension\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dbrlzg5vy8a",
   "source": [
    "# Train the model (using the same generic training function!)\n",
    "# Note: CIFAR-10 is more complex than MNIST, may need longer training\n",
    "# The training will automatically RESUME from the latest checkpoint if interrupted!\n",
    "train_denoiser(\n",
    "    model=model,\n",
    "    data=cifar_data,\n",
    "    noise_schedule=schedule,\n",
    "    epochs=100_000,\n",
    "    lr=1e-3,\n",
    "    batch_size=128,\n",
    "    checkpoint_dir=\"checkpoints/cifar10_cnn\",\n",
    "    save_every=1_000,\n",
    "    resume=True,\n",
    ")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "v2li4bse91",
   "source": "### üîÑ Auto-Resume Feature\n\nThe training will **automatically resume** from the latest checkpoint if interrupted! This is perfect for:\n- ‚è∞ Google Colab sessions that time out\n- üîå Unexpected disconnections\n- üõë Manual interruptions\n\nJust re-run the training cell and it will pick up where it left off. No manual checkpoint management needed!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "rzhqpscgrv",
   "source": "## Test the Reverse Diffusion Process\n\nLet's test our trained model by generating RGB images from pure noise! This will verify that all interfaces work correctly for CIFAR-10 before we move to long training on Google Colab.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "n35prg5ucj",
   "source": [
    "from src.diffusion_playground.training.denoiser_trainer import load_checkpoint\n",
    "\n",
    "# Load checkpoint for testing\n",
    "cp_name = \"checkpoint_epoch_90000.pt\"\n",
    "checkpoint_path = f\"checkpoints/cifar10_cnn/{cp_name}\"\n",
    "\n",
    "# Create a fresh model instance\n",
    "model = CNNDenoiser(in_channels=3, base_channels=64, time_emb_dim=128)\n",
    "model.to(device)\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint_info = load_checkpoint(model, checkpoint_path, device=device)\n",
    "print(f\"Loaded model trained for {checkpoint_info['epoch']} epochs\")\n",
    "print(f\"Training loss: {checkpoint_info['loss']:.6f}\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "g24os5v85bm",
   "source": [
    "# Setup for generation\n",
    "model.eval()\n",
    "num_samples = 9\n",
    "\n",
    "images = generate_samples(\n",
    "    model=model,\n",
    "    noise_schedule=schedule,\n",
    "    image_shape=(3, 32, 32),\n",
    "    num_samples=num_samples,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Visualize generated RGB samples\n",
    "fig, axes = plt.subplots(3, 3, figsize=(8, 8))\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    # Convert from [-1, 1] to [0, 1] for visualization\n",
    "    img = images[idx]\n",
    "\n",
    "    ax.imshow(img.cpu())\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# Create informative title with checkpoint details\n",
    "title = f\"Generated CIFAR-10 Images - {cp_name}\\n\"\n",
    "title += f\"Epoch: {checkpoint_info['epoch']} | Loss: {checkpoint_info['loss']:.6f}\"\n",
    "plt.suptitle(title, fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Generate Documentation Visualizations\n",
    "\n",
    "Create and save visualizations for multiple checkpoints to use in the README documentation."
   ],
   "id": "d63923e2f25a67a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate the samples\n",
    "generate_samples_from_checkpoints(\n",
    "    model=model,\n",
    "    dataset_name_trained_on=\"CIFAR-10\",\n",
    "    device=\"cpu\",\n",
    "    checkpoint_epochs=[1000, 25000, 50000, 75000],\n",
    "    checkpoint_dir=\"./checkpoints/cifar10_cnn\",\n",
    "    output_dir=\"../../../docs/cifar-10-cnn\",\n",
    "    noise_schedule=schedule,\n",
    "    image_shape=(3, 32, 32),\n",
    ")"
   ],
   "id": "c96f9e62bfdfffb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d6236e00c9b1f0ef",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
