{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Diffusion on the Moons üåó Toy-Dataset - Basic MLP PoC\n",
    "\n",
    "This notebook demonstrates a **proof-of-concept diffusion model** trained on the classic scikit-learn \"moons\" toy dataset. Diffusion models work by learning to reverse a gradual noising process: we start with clean data, progressively add Gaussian noise over multiple time steps, and then train a neural network to predict and remove that noise step-by-step.\n",
    "\n",
    "In this experiment, we use a simple **Multi-Layer Perceptron (MLP)** as our denoising model to learn the reverse diffusion process on 2D point clouds. This toy example helps visualize how diffusion models generate new samples by starting from pure noise and iteratively denoising to recover the data distribution."
   ],
   "id": "ad75104a14973378"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.diffusion_playground.visualization.plot import show_denoising_steps_2d\n",
    "from src.diffusion_playground.data_loader.toy_datasets import load_toy_dataset\n",
    "from src.diffusion_playground.models.mlp_denoiser import MLPDenoiser\n",
    "from src.diffusion_playground.diffusion.noise_schedule import LinearNoiseSchedule\n",
    "from src.diffusion_playground.training.denoiser_trainer import train_denoiser"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train the Model üèãÔ∏è\n",
    "\n",
    "Here we set up and train our MLP denoiser on the moons dataset. The training process teaches the model to predict the noise that was added at each time step during the forward diffusion process.\n",
    "\n",
    "**Key components:**\n",
    "- **Dataset**: 1,000 samples from the moons toy dataset (two interleaving half-circles)\n",
    "- **Model**: A simple MLP that takes noisy data points and the current time step as input\n",
    "- **Noise Schedule**: Linear schedule over 100 time steps controlling how noise is added\n",
    "- **Training**: 100,000 epochs to learn the noise prediction task (takes ~1 minute on a typical laptop's CPU)"
   ],
   "id": "add67c899caa5090"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the data\n",
    "data = load_toy_dataset(\"moons\", n_samples=1_000)\n",
    "data = torch.tensor(data)\n",
    "\n",
    "# Create the model\n",
    "model = MLPDenoiser()\n",
    "\n",
    "# Create the noising schedule\n",
    "schedule = LinearNoiseSchedule(time_steps=100)\n",
    "\n",
    "# Train the model\n",
    "# Note: This should not take longer than approx. 1 minute on a normal laptop\n",
    "train_denoiser(model, data, schedule, epochs=100_000)"
   ],
   "id": "37f47d7880da11d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluate the Model üìà\n",
    "\n",
    "Now that our model is trained, let's test its ability to generate new samples! We'll start from pure Gaussian noise and iteratively denoise it using our trained MLP. If successful, the final denoised samples should resemble the characteristic moon shapes from our training data.\n",
    "\n",
    "We'll visualize both the original data (in red) and track the denoising process at various time steps to see how the model gradually recovers the data distribution from random noise."
   ],
   "id": "298fbee197039b3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 256\n",
    "time_steps = 100\n",
    "visualize_ever = 10\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Get and show the original moons data for comparison with our generated samples\n",
    "idx = torch.randint(0, data.shape[0], (batch_size,))\n",
    "x0_eval = data[idx]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x0_eval[:, 0], x0_eval[:, 1], c=\"red\", alpha=0.6)\n",
    "plt.title(\"Sample from the original Moons Dataset (Ground Truth)\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"x‚ÇÄ\")\n",
    "plt.ylabel(\"x‚ÇÅ\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "id": "d312ba9cec854fa5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### De-Noising Process ‚öôÔ∏è\n",
    "\n",
    "This is the **reverse diffusion process** - the heart of generation! We start with pure random noise and step backwards through our 100 time steps, using our trained model to predict and remove noise at each step.\n",
    "\n",
    "At each iteration:\n",
    "1. The model predicts what noise was added at time step `t`\n",
    "2. We use the noise schedule parameters (Œ±, Œ≤) to calculate the cleaner version of the data\n",
    "3. We step backwards in time, gradually revealing the underlying moon pattern\n",
    "\n",
    "We save snapshots every 10 steps to visualize how the structure emerges from chaos."
   ],
   "id": "b7d83f2d8776d43c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create pure noise as a starting point\n",
    "xt = torch.randn_like(x0_eval).to(device)\n",
    "x_steps = []\n",
    "\n",
    "# De-noising loop\n",
    "for t in reversed(range(1, time_steps + 1)):\n",
    "    # The current time step for every datapoint (same for all datapoints)\n",
    "    t_tensor = torch.full((batch_size, 1), t, device=device, dtype=torch.float32)\n",
    "\n",
    "    # Predict the noise that was \"added\" at this time step - Reverse diffusion process\n",
    "    with torch.no_grad():\n",
    "        pred_noise = model(xt, t_tensor)\n",
    "\n",
    "    # Re-calculate the previous datapoints (note that the weights alpha_hat and beta must be included!)\n",
    "    beta_t = schedule.betas[t - 1]\n",
    "    alpha_t = schedule.alphas[t - 1]\n",
    "    alpha_bar_t = schedule.alpha_bars[t - 1]\n",
    "\n",
    "    # Step back\n",
    "    x_prev = (xt - beta_t / torch.sqrt(1 - alpha_bar_t) * pred_noise) / torch.sqrt(alpha_t)\n",
    "\n",
    "    # Update xt\n",
    "    xt = x_prev\n",
    "\n",
    "    # Track steps for visualization\n",
    "    if t % visualize_ever == 0 or t == 1:\n",
    "        x_steps.append((xt.clone(), pred_noise.clone()))"
   ],
   "id": "bc65c07a4ed354ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Visualize the Interim De-Noising Results üèûÔ∏è\n",
    "\n",
    "Watch the magic happen! These visualizations show how our generated samples evolve during the reverse diffusion process:\n",
    "\n",
    "- **Step 0/100** (t=100): Starting point - pure random noise with no discernible structure\n",
    "- **Step 50/100** (t=50): Midway through - some structure begins to emerge\n",
    "- **Step 100/100** (t=1): Final result - clean samples that should closely match the moon distribution"
   ],
   "id": "27601e735f670aa3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "show_denoising_steps_2d(x0_eval, *x_steps[0], title=\"Step 0 / 100\")",
   "id": "b6079769edd92f9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "show_denoising_steps_2d(x0_eval, *x_steps[5], title=\"Step 50 / 100\")",
   "id": "5db85f1bf3232bec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "show_denoising_steps_2d(x0_eval, *x_steps[9], title=\"Step 100 / 100\")",
   "id": "faf4af65d18b5db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Print the Model Summary and Final Loss Value üíØ\n",
    "\n",
    "Let's evaluate the quality of our generated samples by computing the Mean Squared Error (MSE) between the final denoised samples and the original data. A low MSE indicates that our model successfully learned to generate samples similar to the training distribution.\n",
    "\n",
    "We also display the model architecture summary to see the simplicity of our MLP denoiser - proof that even simple models can learn diffusion dynamics on toy datasets!"
   ],
   "id": "b5eea51564d2fa44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mse_final = F.mse_loss(xt, x0_eval)\n",
    "print(f\"Final MSE: {mse_final.item():.4f}\")\n",
    "\n",
    "summary(model)"
   ],
   "id": "b3c4c8900139b132",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "cd94f43c0c965b5d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
